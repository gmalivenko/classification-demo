export default {
    models: [    
        {
            name: 'resnet-10',
            full_name: 'ResNet-10',
            description: 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',
            paper: 'https://arxiv.org/abs/1512.03385',
            weights_src: ['models/resnet10/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '34.69%',
            reported_top5_error: '14.36%',
            flops: '894.04M',
            num_params: '5,418,792',
        },
        {
            name: 'resnet-18',
            full_name: 'ResNet-18',
            description: 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',
            paper: 'https://arxiv.org/abs/1512.03385',
            weights_src: ['models/resnet18/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '28.53%',
            reported_top5_error: '9.82%',
            flops: '1,820.41M',
            num_params: '11,689,512',
        },

        {
            name: 'resnet-34',
            full_name: 'ResNet-34',
            description: 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',
            paper: 'https://arxiv.org/abs/1512.03385',
            weights_src: ['models/resnet34/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '24.84%',
            reported_top5_error: '7.80 %',
            flops: '3,672.68M',
            num_params: '21,797,672',
        },

        {
            name: 'resnet-50',
            full_name: 'ResNet-50',
            description: 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',
            paper: 'https://arxiv.org/abs/1512.03385',
            weights_src: ['models/resnet50/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '22.28%',
            reported_top5_error: '6.33%',
            flops: '3,877.95M',
            num_params: '25,557,032 ',
        },

        {
            name: 'inception-v3',
            full_name: 'inception-v3',
            description: '(bad quality, todo)\n Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.',
            paper: 'https://arxiv.org/abs/1512.00567',
            weights_src: ['models/inceptionv3/model.json'],
            window_size: 299,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '21.12%',
            reported_top5_error: '5.65%',
            flops: '5,743.06M',
            num_params: '23,834,568',
        },

        {
            name: 'preresnet-18',
            full_name: 'preresnet-18',
            description: 'Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.',
            paper: 'https://arxiv.org/abs/1603.05027',
            weights_src: ['models/preresnet18/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '28.43%',       
            reported_top5_error: '9.72%',
            flops: '1,820.56M',
            num_params: '11,687,848',
        },

        {
            name: 'preresnet-34',
            full_name: 'preresnet-34',
            description: 'Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.',
            paper: 'https://arxiv.org/abs/1603.05027',
            weights_src: ['models/preresnet34/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '24.89%',       
            reported_top5_error: '7.74%',
            flops: '3,672.83M',
            num_params: '21,796,008',
        },

        {
            name: 'preresnet-50',
            full_name: 'preresnet-50',
            description: 'Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.',
            paper: 'https://arxiv.org/abs/1603.05027',
            weights_src: ['models/preresnet50/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '22.40%',       
            reported_top5_error: '6.47',
            flops: '3,875.44M ',
            num_params: '25,549,480',
        }, 

        {             
            name: 'densenet-121',
            full_name: 'DenseNet-121',
            description: 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance.',
            paper: 'https://arxiv.org/abs/1608.06993',
            weights_src: ['models/densenet121/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '23.48%',
            reported_top5_error: '7.04%',
            flops: '2,872.13M',
            num_params: '7,978,856',
        },

        {             
            name: 'resnext14_32x4d',
            full_name: 'resnext 14_32x4d',
            description: 'We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.',
            paper: 'https://arxiv.org/abs/1611.05431',
            weights_src: ['models/resnext14_32x4d/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '30.32%',
            reported_top5_error: '11.46%',
            flops: '1,603.46M',
            num_params: '9,411,880',
        },

        {             
            name: 'resnext26_32x4d',
            full_name: 'resnext 26_32x4d',
            description: 'We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.',
            paper: 'https://arxiv.org/abs/1611.05431',
            weights_src: ['models/resnext26_32x4d/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '24.14%',
            reported_top5_error: '7.46%',
            flops: '2,488.07M',
            num_params: '15,389,480',
        },

        {             
            name: 'darknet_ref',
            full_name: 'darknet_ref',
            description: 'darknet_ref model',
            paper: 'https://arxiv.org/abs/1611.05431',
            weights_src: ['models/darknet_ref/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '38.58%',
            reported_top5_error: '17.18%',
            flops: '367.59M',
            num_params: '7,319,416',
        },

        {             
            name: 'darknet_tiny',
            full_name: 'darknet_tiny',
            description: 'darknet_tiny model',
            paper: 'https://arxiv.org/abs/1611.05431',
            weights_src: ['models/darknet_tiny/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '40.74%',
            reported_top5_error: '17.84%',
            flops: '500.85M',
            num_params: '1,042,104',
        },
        
        {
            name: 'darknet53',
            full_name: 'darknet53',
            description: 'We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that\'s pretty swell. It\'s a little bigger than last time but more accurate. It\'s still fast though, don\'t worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster.',
            paper: 'https://arxiv.org/abs/1804.02767',
            weights_src: ['models/darknet53/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '21.75%',
            reported_top5_error: '5.64%',
            flops: '7,133.86M',
            num_params: '41,609,928',
        }, 

        {
            name: 'dpn68',
            full_name: 'dpn68',
            description: 'In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.',
            paper: 'https://arxiv.org/abs/1707.01629',
            weights_src: ['models/dpn68/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '23.24%',
            reported_top5_error: '6.79%',
            flops: '2,351.84M',
            num_params: '12,611,602',
        }, 

        {
            name: 'shufflenet_g1_w1',
            full_name: 'shufflenet g1_w1',
            description: 'We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.',
            paper: 'https://arxiv.org/abs/1707.01083',
            weights_src: ['models/shufflenet_g1_w1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '34.93%',
            reported_top5_error: '13.89%',
            flops: '148.13M',
            num_params: '1,531,936',
        },

        {
            name: 'diracnet18v2',
            full_name: 'diracnet18v2',
            description: 'Deep neural networks with skip-connections, such as ResNet, show excellent performance in various image classification benchmarks. It is though observed that the initial motivation behind them - training deeper networks - does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without explicit skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference, as both Dirac parameterization and batch normalization can be folded into convolutional filters, so that network becomes a simple chain of convolution-ReLU pairs. We are able to match ResNet-1001 accuracy on CIFAR-10 with 28-layer wider plain DiracNet, and closely match ResNets on ImageNet. Our parameterization also mostly eliminates the need of careful initialization in residual and non-residual networks. ',
            paper: 'https://arxiv.org/abs/1706.00388',
            weights_src: ['models/diracnet18v2/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '31.47%',
            reported_top5_error: '11.70%',
            flops: '1,796.62M',
            num_params: '11,511,784',        
        },

        {
            name: 'diracnet34v2',
            full_name: 'diracnet34v2',
            description: 'Deep neural networks with skip-connections, such as ResNet, show excellent performance in various image classification benchmarks. It is though observed that the initial motivation behind them - training deeper networks - does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without explicit skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference, as both Dirac parameterization and batch normalization can be folded into convolutional filters, so that network becomes a simple chain of convolution-ReLU pairs. We are able to match ResNet-1001 accuracy on CIFAR-10 with 28-layer wider plain DiracNet, and closely match ResNets on ImageNet. Our parameterization also mostly eliminates the need of careful initialization in residual and non-residual networks. ',
            paper: 'https://arxiv.org/abs/1706.00388',
            weights_src: ['models/diracnet34v2/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '28.75%',
            reported_top5_error: '9.93%',
            flops: '3,646.93M',
            num_params: '21,616,232',        
        },

        {
            name: 'senet-16',
            full_name: 'senet-16',
            description: 'The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.',
            paper: 'https://arxiv.org/abs/1709.01507',
            weights_src: ['models/senet16/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '25.65%',
            reported_top5_error: '8.20%',
            flops: '5,081.30M',
            num_params: '31,366,168',        
        },

        {
            name: 'mobilenet_w1',
            full_name: 'mobilenet_w1',
            description: 'We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.',
            paper: 'https://arxiv.org/abs/1704.04861',
            weights_src: ['models/mobilenet_w1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '26.61%',
            reported_top5_error: '8.95%',
            flops: '579.80M',
            num_params: '4,231,976',        
        },

        {
            name: 'nasnet_4a1056',
            full_name: 'nasnet 4a1056',
            description: 'Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.',
            paper: 'https://arxiv.org/abs/1707.07012',
            weights_src: ['models/nasnet_4a1056/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '25.68%',
            reported_top5_error: '8.16%',
            flops: '584.90M',
            num_params: '5,289,978',        
        },    

        {
            name: 'dla34',
            full_name: 'dla34',
            description: 'Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.',
            paper: 'https://arxiv.org/abs/1707.06484',
            weights_src: ['models/dla34/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '25.36%',
            reported_top5_error: '7.94%',
            flops: '3,071.37M',
            num_params: '15,742,104',
        }, 

        {
            name: 'squeezeresnet_v1_1',
            full_name: 'squeezeresnet v1.1',
            description: 'Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). ',
            paper: 'https://arxiv.org/abs/1602.07360',
            weights_src: ['models/squeezeresnet_v1_1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '40.09%',       
            reported_top5_error: '18.21%',
            flops: '352.02M',
            num_params: '1,235,496',
        },

        {
            name: 'squeezenet_v1_1',
            full_name: 'squeezenet v1.1',
            description: 'Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). ',
            paper: 'https://arxiv.org/abs/1602.07360',
            weights_src: ['models/squeezenet_v1_1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '40.09%',
            reported_top5_error: '18.21%',
            flops: '352.02M',
            num_params: '1,235,496',
        },

        {
            name: 'shufflenetv2_w1',
            full_name: 'shufflenetv2',
            description: 'Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.',
            paper: 'https://arxiv.org/abs/1807.11164',
            weights_src: ['models/shufflenetv2_w1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '31.44%',
            reported_top5_error: '11.63%',
            flops: '149.72M',
            num_params: '2,278,604',
        },

        {
            name: 'fdmobilenet_w1',
            full_name: 'fdmobilenet w1',
            description: 'We present Fast-Downsampling MobileNet (FD-MobileNet), an efficient and accurate network for very limited computational budgets (e.g., 10-140 MFLOPs). Our key idea is applying an aggressive downsampling strategy to MobileNet framework. In FD-MobileNet, we perform 32× downsampling within 12 layers, only half the layers in the original MobileNet. This design brings three advantages: (i) It remarkably reduces the computational cost. (ii) It increases the information capacity and achieves significant performance improvements. (iii) It is engineering-friendly and provides fast actual inference speed. Experiments on ILSVRC 2012 and PASCAL VOC 2007 datasets demonstrate that FD-MobileNet consistently outperforms MobileNet and achieves comparable results with ShuffleNet under different computational budgets, for instance, surpassing MobileNet by 5.5% on the ILSVRC 2012 top-1 accuracy and 3.6% on the VOC 2007 mAP under a complexity of 12 MFLOPs. On an ARM-based device, FD-MobileNet achieves 1.11× inference speedup over MobileNet and 1.82× over ShuffleNet under the same complexity.',
            paper: 'https://arxiv.org/abs/1802.03750',
            weights_src: ['models/fdmobilenet_w1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '34.23%',       
            reported_top5_error: '13.38%',
            flops: '147.46M',
            num_params: '2,901,288',       
        }, 

        {
            name: 'mobilenetv2_w1',
            full_name: 'mobilenetv2 w1',
            description: 'In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power.',
            paper: 'https://arxiv.org/abs/1801.04381',
            weights_src: ['models/mobilenetv2_w1/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '26.97%',
            reported_top5_error: '8.87%',
            flops: '329.36M',
            num_params: '3,504,960',
        },

        {
            name: 'mnasnet',
            full_name: 'mnasnet',
            description: 'Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection.',
            paper: 'https://arxiv.org/abs/1807.11626',
            weights_src: ['models/mnasnet/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '31.58%',
            reported_top5_error: '11.74%',
            flops: '317.67M',
            num_params: '4,308,816',
        },    

        {
            name: 'efficientnet_b0',
            full_name: 'efficientnet b0',
            description: 'Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.',
            paper: 'https://arxiv.org/abs/1905.11946',
            weights_src: ['models/efficientnet_b0/model.json'],
            window_size: 224,
            mean: [0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0],
            std: [0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0],
            classes: 'imagenet-1000',
            weights_source: 'osmr',
            reported_top1_error: '24.77%',
            reported_top5_error: '7.52%',
            flops: '414.31M',
            num_params: '5,288,548',
        }, 

        

        

        
        
        

        

        
    ]     

}